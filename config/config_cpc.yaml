# distributed training
nodes: 1
gpus: 1 # I recommend always assigning 1 GPU to 1 node
nr: 0 # machine nr. in node (0 -- nodes - 1)
workers: 8

## task / dataset options
domain: "audio" # [audio,scores]
task: "tags" # [tags,chords,symbol]
dataset: "msd" # [audio: [billboard,fma,magnatagatune,msd] vision: [deepscores,universal]]
data_input_dir: "/storage/jspijkervet" # [audio: [datasets/audio] vision: [datasets/vision]
pretrain_dataset: "msd" # [magnatagatune,billboard,fma,msd]
model_name: "clmr" # [clmr, cpc]

## train options
seed: 42 # sacred handles automatic seeding when passed in the config
batch_size: 48 # for audio, 256 for images
num_workers: 12
start_epoch: 0
epochs: 100
checkpoint_epochs: 10

## loss options
optimizer: "Adam" # [Adam, LARS]
learning_rate: 3.0e-4 # for Adam optimizer, LARS uses batch-specific LR
weight_decay: 1.0e-6
temperature: 0.5 # (NOTE: 0.1 goes very fast) see appendix B.7.: Optimal temperature under different batch sizes

## supervised params
supervised: False # to train encoder in fully supervised fashion

## model options
normalize: True
projection_dim: 128
projector_layers: 2
dropout: 0.5

## audio
audio_length: 59049
sample_rate: 22050

## CPC
prediction_step: 20 # Time steps k to predict into future
negative_samples: 15 # Number of negative samples to be used for training
subsample: True # Boolean to decide whether to subsample from the total sequence lengh within intermediate layers


## CLMR
### transformations
transforms_phase: 0.8
transforms_noise: 0.0
transforms_gain: 0.1
transforms_filters: 0.4
transforms_highpass_freq: 800
transforms_lowpass_freq: 3500


## reload options
model_path: "save" # set to the directory containing `checkpoint_##.pt` 
epoch_num: 0 # set to checkpoint number
reload: False


## linear evaluation options
mlp: True 
logistic_batch_size: 64
logistic_epochs: 100
logistic_lr: 0.003 # clmr: 0.0004, cpc: 0.003
reload_logreg: False
perc_train_data: 1.0
finetune_model_path: ""
finetune_epoch_num: ""
num_tags: 50
